{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cipB14/Questify/blob/main/Review3_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Libraries and Their Use in Questify Project\n",
        "\n",
        "| Library/Tool        | Use Case in Questify                                                                       |\n",
        "|---------------------|----------------------------------------------------------------------|\n",
        "| pdf4llm           | Extracts structured content (text, images, tables) from PDFs         |\n",
        "| transformers      | Loads and runs LLMs (e.g., Mistral for question generation, BERT for classification) |\n",
        "| accelerate        | Speeds up model inference across GPU/CPU environments                |\n",
        "| bitsandbytes      | Enables low-bit quantization for memory-efficient LLMs               |\n",
        "| lancedb           | Stores SBERT embeddings for hybrid search of study content           |\n",
        "| tantivy           | Provides fast keyword-based full-text indexing and search            |\n",
        "\n",
        "\n",
        "| Model Name                     | Type                          | Layers | Max Seq Length | Use Case                              | Labels / Output      |\n",
        "|-------------------------------|-------------------------------|--------|----------------|----------------------------------------|-----------------------|\n",
        "| Mistral-7B-Instruct-v0.3      | MistralForCausalLM            | 32     | 32,768         | Question Generation                    | Text (Generated Qs)   |\n",
        "| all-MiniLM-L6-v2              | BertModel                     | 6      | 512            | Sentence Embeddings for Retrieval      | Embeddings            |\n",
        "| ms-marco-TinyBERT-L6          | BertForSequenceClassification | 6      | 512            | Passage Reranking                      | Relevance Score       |\n",
        "| cip29/blooms_bert             | BertForSequenceClassification | 12     | 512            | Bloom’s Taxonomy Classification        | 6 Bloom’s Levels      |\n"
      ],
      "metadata": {
        "id": "c-tzXyk93GTu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zu_eYt3m2YtS"
      },
      "outputs": [],
      "source": [
        "!pip install -qU  pymupdf4llm transformers accelerate bitsandbytes tantivy gradio lancedb==0.20.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login\n",
        "#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"
      ],
      "metadata": {
        "id": "MgPiLXwO5R52"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BertTokenizer, BertForSequenceClassification, BitsAndBytesConfig\n",
        "\n",
        "#Enable 4-bit Quantization\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,  # Use 4-bit precision\n",
        "    bnb_4bit_compute_dtype=\"float16\",  # Use float16 for faster computation\n",
        "    bnb_4bit_use_double_quant=True,  # Improves efficiency\n",
        "    bnb_4bit_quant_type=\"nf4\"  # NF4 quantization for better accuracy\n",
        ")\n",
        "\n",
        "#Load Tokenizer & Model with Quantization\n",
        "mistral_tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.3\")\n",
        "\n",
        "mistral_model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"mistralai/Mistral-7B-Instruct-v0.3\",\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"cuda\"  # Automatically assigns model to GPU\n",
        ")\n",
        "\n",
        "#Load Bloom’s Taxonomy BERT Model\n",
        "blooms_model_name = \"cip29/blooms_bert\"\n",
        "blooms_tokenizer = BertTokenizer.from_pretrained(blooms_model_name)\n",
        "blooms_model = BertForSequenceClassification.from_pretrained(blooms_model_name, num_labels=6).to(\"cuda\")"
      ],
      "metadata": {
        "id": "jwV_TcMo3FNJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### sentence-transformers/all-MiniLM-L6-v2\n",
        "{\n",
        "  \"max_seq_length\": 256,\n",
        "  \"do_lower_case\": false\n",
        "}\n",
        "### Tokenizer Config\n",
        "{\n",
        "  \"do_lower_case\": true, \"unk_token\": \"[UNK]\", \"sep_token\": \"[SEP]\", \"pad_token\": \"[PAD]\", \"cls_token\": \"[CLS]\", \"mask_token\": \"[MASK]\", \"tokenize_chinese_chars\": true, \"strip_accents\": null, \"name_or_path\": \"nreimers/MiniLM-L6-H384-uncased\", \"do_basic_tokenize\": true, \"never_split\": null, \"tokenizer_class\": \"BertTokenizer\", \"model_max_length\": 512\n",
        "  }\n",
        "\n",
        "  This is a sentence-transformers model: It maps sentences & paragraphs to a 384 dimensional dense vector space and can be used for tasks like clustering or semantic search."
      ],
      "metadata": {
        "id": "gD6aNiOLcTSn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "######  pdf4llm.to_markdown()\n",
        "outputs a list of dictinoaries containing\n",
        "\n",
        "- metadata:\n",
        "  - format: \"Image\"\n",
        "  - title: \"\"\n",
        "  - author: \"\"\n",
        "  - subject: \"\"\n",
        "  - keywords: \"\"\n",
        "  - creator: \"\"\n",
        "  - producer: \"\"\n",
        "  - creationDate: \"\"\n",
        "  - modDate: \"\"\n",
        "  - trapped: \"\"\n",
        "  - encryption: \"\"\n",
        "  - file_path: \"1-2.png\"\n",
        "  - page_count: 1\n",
        "  - page: 1\n",
        "\n",
        "- toc_items: []\n",
        "\n",
        "- tables: []\n",
        "\n",
        "- images:\n",
        "  - number: 0\n",
        "  - bbox: Rect(0.0, 50.0, 648.0, 310.0)\n",
        "  - transform: (648.0, 0.0, 0.0, 360.0, 0.0, 0.0)\n",
        "  - width: 2700\n",
        "  - height: 1500\n",
        "  - colorspace: 3\n",
        "  - cs-name: \"DeviceRGB\"\n",
        "  - xres: 300\n",
        "  - yres: 300\n",
        "  - bpc: 8\n",
        "  - size: 88631\n",
        "  - has-mask: False\n",
        "\n",
        "- graphics: []\n",
        "\n",
        "- text: \"-----\"\n",
        "\n",
        "- words: []\n"
      ],
      "metadata": {
        "id": "sLcz-O6O-OnO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /content/*.pdf\n",
        "\n",
        "import pymupdf4llm\n",
        "from google.colab import files\n",
        "import numpy as np\n",
        "import lancedb\n",
        "from lancedb.embeddings import get_registry\n",
        "from lancedb.pydantic import LanceModel, Vector\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Connect to LanceDB\n",
        "db = lancedb.connect(\"/content\")\n",
        "\n",
        "# Initialize SBERT Embedder\n",
        "embedder = get_registry().get(\"huggingface\").create(\n",
        "    name='sentence-transformers/all-MiniLM-L6-v2',\n",
        "    device=\"cuda\"\n",
        ")\n",
        "\n",
        "# Load tokenizer to chunk text\n",
        "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
        "\n",
        "\n",
        "class PDFSchema(LanceModel):\n",
        "    text: str = embedder.SourceField()              # chunk text (embedding input)\n",
        "    vector: Vector(embedder.ndims()) = embedder.VectorField()\n",
        "    page_name: str                                  # image/visual ID\n",
        "    full_text: str                                   # full page text for reference\n",
        "    page: int                                        # page number to detect duplicates\n",
        "\n",
        "\n",
        "\n",
        "# Upload PDFs\n",
        "uploaded = files.upload()\n",
        "print(list(uploaded.keys()))\n"
      ],
      "metadata": {
        "id": "Wkhrqz9E5jWc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Including overlap between chunks helps preserve context across chunk boundaries — especially useful when text is split in the middle of a sentence or paragraph(sliding window approach)\n",
        "\n"
      ],
      "metadata": {
        "id": "PaJece4Ag4xK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define chunking parameters\n",
        "CHUNK_SIZE = 448\n",
        "OVERLAP = 32\n",
        "\n",
        "# Function to process and chunk text with a sliding window\n",
        "def split_text_into_chunks(text, page_path, full_text, page_number):\n",
        "    input_ids = tokenizer.encode(text, truncation=False, add_special_tokens=False)\n",
        "    chunks = []\n",
        "\n",
        "    for i in range(0, len(input_ids), CHUNK_SIZE - OVERLAP):\n",
        "        chunk_ids = input_ids[i:i + CHUNK_SIZE]\n",
        "\n",
        "        if len(chunk_ids) < 10:  # Skip very small chunks\n",
        "            continue\n",
        "\n",
        "        chunk_text = tokenizer.decode(chunk_ids, skip_special_tokens=True)\n",
        "        chunk_name = f\"{page_path}_chunk_{i // (CHUNK_SIZE - OVERLAP) + 1}\"\n",
        "\n",
        "        chunks.append({\n",
        "            \"text\": chunk_text,\n",
        "            \"page_name\": chunk_name,\n",
        "            \"full_text\": full_text,\n",
        "            \"page\": page_number\n",
        "        })\n",
        "\n",
        "    return chunks\n",
        "\n",
        "# Collect all entries\n",
        "entries = []\n",
        "\n",
        "# Process each uploaded file\n",
        "uploaded_files = list(uploaded.keys())[:2]  # Limiting to 2 files for processing\n",
        "for pdf_filename in uploaded_files:\n",
        "    print(f\"\\nProcessing: {pdf_filename}\")\n",
        "\n",
        "    # Ask for page numbers or ranges\n",
        "    page_input = input(f\"Enter pages or ranges for {pdf_filename} (e.g., 1,3-5,7): \")\n",
        "\n",
        "    # Parse user input into zero-based page indices\n",
        "    selected_pages = []\n",
        "    for part in page_input.split(\",\"):\n",
        "        part = part.strip()\n",
        "        if \"-\" in part:\n",
        "            start, end = map(int, part.split(\"-\"))\n",
        "            selected_pages.extend(range(start - 1, end))  # Convert to zero-based index\n",
        "        else:\n",
        "            selected_pages.append(int(part) - 1)\n",
        "\n",
        "    # Extract specified pages\n",
        "    selected_page_data = pymupdf4llm.to_markdown(pdf_filename, page_chunks=True, pages=selected_pages)\n",
        "\n",
        "    # Process each page\n",
        "    for page_data in selected_page_data:\n",
        "        full_text = page_data[\"text\"]\n",
        "        page_path = page_data[\"metadata\"][\"file_path\"]\n",
        "        page_number = page_data[\"metadata\"][\"page\"]\n",
        "\n",
        "        if not full_text.strip():  # Skip empty pages\n",
        "            continue\n",
        "\n",
        "        # Split text into overlapping chunks with full page context\n",
        "        chunks = split_text_into_chunks(full_text, page_path, full_text, page_number)\n",
        "\n",
        "        # Add chunks to entries\n",
        "        entries.extend(chunks)\n",
        "\n",
        "# Store all entries in LanceDB\n",
        "tbl = db.create_table(\"pdf_data\", schema=PDFSchema, mode=\"overwrite\")\n",
        "tbl.add(entries)\n",
        "\n",
        "print(\"\\nAll selected pages have been chunked and stored in LanceDB with full page context!\")\n"
      ],
      "metadata": {
        "id": "nbhoPJzC5IeW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from lancedb.rerankers import CrossEncoderReranker\n",
        "\n",
        "# Initialize reranker\n",
        "reranker = CrossEncoderReranker()\n",
        "\n",
        "# User query\n",
        "query = input(\"\\nEnter your query: \")\n",
        "\n",
        "# Create full-text search index on the 'text' field\n",
        "tbl.create_fts_index(\"text\", replace=True)\n",
        "\n",
        "# Search and rerank\n",
        "results = tbl.search(query, query_type=\"hybrid\").rerank(reranker=reranker).limit(5).to_list()\n",
        "\n",
        "# Dictionary to hold unique pages\n",
        "unique_pages = {}\n",
        "\n",
        "# Filter out duplicates using the 'page' key\n",
        "for res in results:\n",
        "    page_number = res.get(\"page\")\n",
        "    if page_number not in unique_pages:\n",
        "        unique_pages[page_number] = res[\"full_text\"]\n",
        "\n",
        "# Final list of unique full_text values with page number\n",
        "final_full_texts = [{\"page\": page, \"text\": text} for page, text in unique_pages.items()]\n",
        "\n",
        "# Optional: Display them\n",
        "print(\"\\nUnique full_text entries by page:\\n\")\n",
        "for i, entry in enumerate(final_full_texts, 1):\n",
        "    print(f\"[{i}] Page {entry['page']}:\\n{entry['text'][:500]}...\\n{'-'*80}\")\n"
      ],
      "metadata": {
        "id": "Wu-ATHJs9_Zk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### QUESTION DIVERSITY EVALUATION\n"
      ],
      "metadata": {
        "id": "1_4Jk8yQEEIs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "sentence_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "# Generate multiple question sets\n",
        "def generate_multiple_question_sets(pages_text, user_query, num_sets=5):\n",
        "    question_sets = []\n",
        "\n",
        "    for _ in range(num_sets):\n",
        "        prompt = f\"\"\"\n",
        "You are an AI assistant specialized in question generation.\n",
        "Your goal is to generate insightful questions based on the given context and user query.\n",
        "\n",
        "Context:\n",
        "{pages_text}\n",
        "\n",
        "User Query (Focus Topic): {user_query}\n",
        "\n",
        "### Reasoning:\n",
        "- Step 1: Identify key points and concepts from the context relevant to the query\n",
        "- Step 2: Consider what types of questions best explore the topic of interest\n",
        "- Step 3: Formulate meaningful and topic-specific questions\n",
        "\n",
        "Generate atleast **15 questions** from the context\n",
        "\n",
        "### Questions:\n",
        "\"\"\"\n",
        "        inputs = mistral_tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=4096).to(\"cuda\")\n",
        "        output = mistral_model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=2048,\n",
        "            temperature=0.8,\n",
        "            top_p=0.8,\n",
        "            do_sample=True,\n",
        "            pad_token_id=mistral_tokenizer.eos_token_id\n",
        "        )\n",
        "        generated_text = mistral_tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "        questions = generated_text.split(\"### Questions:\")[-1].strip().split(\"\\n\")\n",
        "        print(questions)\n",
        "        questions = [q.strip(\"- \").strip() for q in questions if q.strip()]\n",
        "        question_sets.append(questions)\n",
        "\n",
        "\n",
        "    return question_sets\n",
        "\n",
        "# Compute pairwise similarities\n",
        "def compute_similarity_between_sets(question_sets):\n",
        "    similarities = []\n",
        "    for i in range(len(question_sets)):\n",
        "        for j in range(i + 1, len(question_sets)):\n",
        "            set1_embeddings = sentence_model.encode(question_sets[i], convert_to_tensor=True)\n",
        "            set2_embeddings = sentence_model.encode(question_sets[j], convert_to_tensor=True)\n",
        "\n",
        "            sim_matrix = util.pytorch_cos_sim(set1_embeddings, set2_embeddings)\n",
        "            avg_sim1 = sim_matrix.max(dim=1).values.mean().item()\n",
        "            avg_sim2 = sim_matrix.max(dim=0).values.mean().item()\n",
        "            avg_pairwise_sim = (avg_sim1 + avg_sim2) / 2\n",
        "            similarities.append(((i, j), avg_pairwise_sim))\n",
        "\n",
        "    return similarities\n",
        "\n",
        "# Plot heatmap\n",
        "def plot_similarity_heatmap(similarities, num_sets):\n",
        "    sim_matrix = np.eye(num_sets)\n",
        "    for (i, j), score in similarities:\n",
        "        sim_matrix[i][j] = sim_matrix[j][i] = score\n",
        "\n",
        "    sns.heatmap(sim_matrix, annot=True, cmap=\"coolwarm\", xticklabels=range(num_sets), yticklabels=range(num_sets))\n",
        "    plt.title(\"Question Set Similarity Heatmap\")\n",
        "    plt.xlabel(\"Set Index\")\n",
        "    plt.ylabel(\"Set Index\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "sets = generate_multiple_question_sets(final_full_texts,query, num_sets=5)\n",
        "similarities = compute_similarity_between_sets(sets)\n",
        "plot_similarity_heatmap(similarities, len(sets))\n"
      ],
      "metadata": {
        "id": "QjrahHUL27JC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mistral-7B-Instruct-v0.3/config.json\n",
        "\n",
        "{\n",
        "  \"architectures\": [\n",
        "    \"MistralForCausalLM\"\n",
        "  ],\n",
        "  \"attention_dropout\": 0.0,\n",
        "  \"bos_token_id\": 1,\n",
        "  \"eos_token_id\": 2,\n",
        "  \"hidden_act\": \"silu\",\n",
        "  \"hidden_size\": 4096,\n",
        "  \"initializer_range\": 0.02,\n",
        "  \"intermediate_size\": 14336,\n",
        "  `\"max_position_embeddings\": 32768,`\n",
        "  \"model_type\": \"mistral\",\n",
        "  \"num_attention_heads\": 32,\n",
        "  \"num_hidden_layers\": 32,\n",
        "  \"num_key_value_heads\": 8,\n",
        "  \"rms_norm_eps\": 1e-05,\n",
        "  \"rope_theta\": 1000000.0,\n",
        "  \"sliding_window\": null,\n",
        "  \"tie_word_embeddings\": false,\n",
        "  \"torch_dtype\": \"bfloat16\",\n",
        "  \"transformers_version\": \"4.42.0.dev0\",\n",
        "  \"use_cache\": true,\n",
        "  \"vocab_size\": 32768\n",
        "}"
      ],
      "metadata": {
        "id": "RJsF8OuTzxu8"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "w1v6L8FEjRUP"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import csv\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from google.colab import files\n",
        "\n",
        "\n",
        "# Bloom's Taxonomy Labels\n",
        "bloom_labels = {\n",
        "    0: \"BT1 (Remembering)\",\n",
        "    1: \"BT2 (Understanding)\",\n",
        "    2: \"BT3 (Applying)\",\n",
        "    3: \"BT4 (Analyzing)\",\n",
        "    4: \"BT5 (Evaluating)\",\n",
        "    5: \"BT6 (Creating)\"\n",
        "}\n",
        "\n",
        "difficulty_labels = {\n",
        "    0: \"Easy\",\n",
        "    1: \"Medium\",\n",
        "    2: \"Hard\"\n",
        "}\n",
        "\n",
        "mark_labels = {\n",
        "    0: \"2 Marks\",\n",
        "    1: \"4 Marks\",\n",
        "    2: \"8 Marks\"\n",
        "}\n",
        "\n",
        "def classify_blooms_taxonomy(question):\n",
        "    inputs = blooms_tokenizer(question, return_tensors=\"pt\", truncation=True, padding=True).to(\"cuda\")\n",
        "    with torch.no_grad():\n",
        "        outputs = blooms_model(**inputs)\n",
        "    probs = F.softmax(outputs.logits, dim=1).squeeze().tolist()\n",
        "    predicted_idx = torch.argmax(outputs.logits, dim=1).item()\n",
        "    predicted_label = bloom_labels[predicted_idx]\n",
        "    prob_dict = {bloom_labels[i]: round(probs[i], 4) for i in range(6)}\n",
        "    return predicted_label, prob_dict\n",
        "\n",
        "def extract_numbered_list(text):\n",
        "    items = re.split(r'\\n(?=\\d+\\.\\s)', text.strip())\n",
        "    return [item.strip() for item in items if item.strip()]\n",
        "\n",
        "def split_number_and_text(item):\n",
        "    match = re.match(r'^(\\d+)\\.\\s+(.*)', item, re.DOTALL)\n",
        "    if match:\n",
        "        return int(match.group(1)), match.group(2).strip()\n",
        "    return None, item.strip()"
      ],
      "metadata": {
        "id": "0RQJzV7ky_zw"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_questions_with_mistral_bulk(pages_text, user_query,no=12):\n",
        "    prompt = f\"\"\"\n",
        "You are an AI assistant specialized in question generation.\n",
        "Your goal is to generate insightful questions based on the given context and user query.\n",
        "\n",
        "Context:\n",
        "{pages_text}\n",
        "\n",
        "User Query (Focus Topic): {user_query}\n",
        "\n",
        "### Reasoning:\n",
        "- Step 1: Identify key points and concepts from the context relevant to the query\n",
        "- Step 2: Consider what types of questions best explore the topic of interest\n",
        "- Step 3: Formulate meaningful and topic-specific questions\n",
        "\n",
        "Generate atleast **{no} questions** from the context\n",
        "\n",
        "### Questions:\n",
        "\"\"\"\n",
        "    inputs = mistral_tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=4096).to(\"cuda\")\n",
        "    output = mistral_model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=2048,\n",
        "        temperature=0.7,\n",
        "        top_p=0.9,\n",
        "        do_sample=True,\n",
        "        pad_token_id=mistral_tokenizer.eos_token_id\n",
        "    )\n",
        "    generated_text = mistral_tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    return generated_text.split(\"### Questions:\")[-1].strip()\n",
        "\n",
        "def write_full_batch_classified_output_to_csv_2(\n",
        "    questions_raw,\n",
        "    context_text,\n",
        "    output_csv=\"final_questions_classified.csv\"\n",
        "):\n",
        "    def extract_numbered_list(text):\n",
        "      pattern = r\"(?:^|\\n)(\\d+)[\\.\\:\\)\\-]\\s*(.+?)(?=\\n\\d+[\\.\\:\\)\\-]|\\Z)\"\n",
        "      matches = re.findall(pattern, text.strip(), flags=re.DOTALL)\n",
        "      return [f\"{num}. {q.strip()}\" for num, q in matches]\n",
        "\n",
        "\n",
        "    # Step 1: Extract numbered question list\n",
        "    question_items = extract_numbered_list(questions_raw)\n",
        "    questions = [split_number_and_text(q)[1].strip() for q in question_items]\n",
        "    numbers = [split_number_and_text(q)[0] for q in question_items]\n",
        "\n",
        "\n",
        "    # Step 2: Batch classify difficulty\n",
        "    bulk_difficulty_prompt = \"\\n\".join(\n",
        "        [f\"{i+1}. {q}\" for i, q in enumerate(questions)]\n",
        "    )\n",
        "    difficulty_prompt = f\"\"\"\n",
        "You are a question analysis assistant.\n",
        "\n",
        "You must classify each question into one of the following difficulty levels based on its complexity, required depth of understanding, and reasoning effort.\n",
        "\n",
        "Guidelines:\n",
        "- **Easy**: Requires simple recall or basic understanding. Usually direct questions that can be answered in a sentence or less.\n",
        "- **Medium**: Requires moderate understanding, application of concepts, or comparison. Involves 2–3 steps of reasoning or synthesis.\n",
        "- **Hard**: Requires deep understanding, critical thinking, and multi-step reasoning. May involve evaluation, derivation, or synthesis of concepts.\n",
        "\n",
        "Now analyze the following:\n",
        "\n",
        "Question:\n",
        "{bulk_difficulty_prompt}\n",
        "\n",
        "### Difficulty levels:\n",
        "\n",
        "\"\"\"\n",
        "    inputs = mistral_tokenizer(difficulty_prompt, return_tensors=\"pt\", truncation=True, max_length=4096).to(\"cuda\")\n",
        "    difficulty_output = mistral_model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=512,\n",
        "        #top_p=0.9,\n",
        "        do_sample=False,\n",
        "        pad_token_id=mistral_tokenizer.eos_token_id\n",
        "    )\n",
        "    difficulty_response = mistral_tokenizer.decode(difficulty_output[0], skip_special_tokens=True).split(\"### Difficulty levels:\")[-1].strip()\n",
        "    print(difficulty_response)\n",
        "    difficulties = [d.strip() for d in extract_numbered_list(difficulty_response)]\n",
        "\n",
        "    # Step 3: Batch classify marks\n",
        "    bulk_marks_prompt = \"\\n\".join(\n",
        "        [f\"{i+1}. {q}\" for i, q in enumerate(questions)]\n",
        "    )\n",
        "    marks_prompt = f\"\"\"\n",
        "You are a question evaluation assistant.\n",
        "\n",
        "You must classify each question into **2, 4, or 8 marks** based on its depth, length of answer required, and reasoning effort.\n",
        "\n",
        "Guidelines:\n",
        "- **2 Marks**: Simple questions requiring one-word or one-line answers. Factual recall.\n",
        "- **4 Marks**: Medium complexity. Conceptual explanation or comparisons in 3-4 sentences.\n",
        "- **8 Marks**: High complexity. Analytical, application-based, or multi-point descriptive answers.\n",
        "\n",
        "Now analyze the following:\n",
        "\n",
        "Question:\n",
        "{bulk_marks_prompt}\n",
        "\n",
        "### Marks Classified:\n",
        "\n",
        "\"\"\"\n",
        "    inputs = mistral_tokenizer(marks_prompt, return_tensors=\"pt\", truncation=True, max_length=4096).to(\"cuda\")\n",
        "    marks_output = mistral_model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=512,\n",
        "        #top_p=0.9,\n",
        "        do_sample=False,\n",
        "        pad_token_id=mistral_tokenizer.eos_token_id\n",
        "    )\n",
        "    marks_response = mistral_tokenizer.decode(marks_output[0], skip_special_tokens=True).split(\"### Marks Classified:\")[-1].strip()\n",
        "    print(marks_response)\n",
        "    marks_list = [m.strip() for m in extract_numbered_list(marks_response)]\n",
        "\n",
        "    # Step 4: Classify Bloom’s taxonomy (batch using your classifier)\n",
        "    blooms_output = [classify_blooms_taxonomy(q) for q in questions]\n",
        "\n",
        "    # Step 5: Compile final rows\n",
        "    all_rows = []\n",
        "    for idx, question in enumerate(questions):\n",
        "        bloom_label, bloom_probs = blooms_output[idx]\n",
        "\n",
        "        difficulty_match = re.search(r\"(Easy|Medium|Hard)\", difficulties[idx], re.IGNORECASE)\n",
        "        difficulty = difficulty_match.group(1).capitalize() if difficulty_match else \"Unknown\"\n",
        "\n",
        "        marks_match = re.search(r\"\\b(2|4|8)\\b\", marks_list[idx])\n",
        "        marks = f\"{marks_match.group(1)} marks\" if marks_match else \"Unknown\"\n",
        "\n",
        "        all_rows.append([\n",
        "            numbers[idx], question, bloom_label, difficulty, marks,\n",
        "            bloom_probs.get(\"BT1 (Remembering)\", 0), bloom_probs.get(\"BT2 (Understanding)\", 0),\n",
        "            bloom_probs.get(\"BT3 (Applying)\", 0), bloom_probs.get(\"BT4 (Analyzing)\", 0),\n",
        "            bloom_probs.get(\"BT5 (Evaluating)\", 0), bloom_probs.get(\"BT6 (Creating)\", 0)\n",
        "        ])\n",
        "\n",
        "    # Step 6: Save to CSV\n",
        "    with open(output_csv, mode='w', newline='', encoding='utf-8') as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow([\n",
        "            \"No\", \"Question\", \"Bloom's Taxonomy Level\", \"Difficulty\", \"Marks\",\n",
        "            \"BT1 (Remembering)\", \"BT2 (Understanding)\", \"BT3 (Applying)\",\n",
        "            \"BT4 (Analyzing)\", \"BT5 (Evaluating)\", \"BT6 (Creating)\"\n",
        "        ])\n",
        "        writer.writerows(all_rows)\n",
        "\n",
        "    print(f\"\\n✅ Saved {len(all_rows)} questions with classification to '{output_csv}'\")\n",
        "    files.download(output_csv)\n",
        "    return output_csv\n"
      ],
      "metadata": {
        "id": "WSOGG6IwyND2"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "no = int(input(\"Enter No of Questions: \"))\n",
        "questions_raw = generate_questions_with_mistral_bulk(final_full_texts,query,no)\n",
        "print(questions_raw)\n",
        "write_full_batch_classified_output_to_csv_2(questions_raw, final_full_texts)\n"
      ],
      "metadata": {
        "id": "DI98GpLoxF69"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load CSV (update path if necessary)\n",
        "df = pd.read_csv(\"/content/final_questions_classified.csv\")\n",
        "\n",
        "\n",
        "# Clean column names\n",
        "df.columns = df.columns.str.strip()\n",
        "\n",
        "# Optional: Normalize \"Marks\" (remove \" marks\" string and convert to int)\n",
        "df['Marks'] = df['Marks'].str.replace(' marks', '').astype(int)\n",
        "\n",
        "# ------------------ PART 1: CATEGORY COUNTS ------------------\n",
        "print(\"🔹 Count by Difficulty:\\n\", df['Difficulty'].value_counts(), \"\\n\")\n",
        "print(\"🔹 Count by Marks:\\n\", df['Marks'].value_counts().sort_index(), \"\\n\")\n",
        "print(\"🔹 Count by Bloom's Taxonomy Level:\\n\", df[\"Bloom's Taxonomy Level\"].value_counts(), \"\\n\")\n",
        "\n",
        "# ------------------ PART 2: DIFFICULTY vs MARKS ------------------\n",
        "matrix_diff_marks = pd.crosstab(df['Difficulty'], df['Marks'])\n",
        "print(\"\\n📊 Matrix: Difficulty vs Marks\\n\", matrix_diff_marks)\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.heatmap(matrix_diff_marks, annot=True, fmt='d', cmap='Blues')\n",
        "plt.title(\"Difficulty vs Marks\")\n",
        "plt.xlabel(\"Marks\")\n",
        "plt.ylabel(\"Difficulty\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ------------------ PART 3: MARKS vs BLOOM ------------------\n",
        "matrix_marks_bloom = pd.crosstab(df['Marks'], df[\"Bloom's Taxonomy Level\"])\n",
        "print(\"\\n📊 Matrix: Marks vs Bloom's Taxonomy\\n\", matrix_marks_bloom)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.heatmap(matrix_marks_bloom, annot=True, fmt='d', cmap='Greens')\n",
        "plt.title(\"Marks vs Bloom's Taxonomy Level\")\n",
        "plt.xlabel(\"Bloom's Taxonomy Level\")\n",
        "plt.ylabel(\"Marks\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "# Extract unique sorted filter values\n",
        "bloom_options = ['Any'] + sorted(df[\"Bloom's Taxonomy Level\"].unique())\n",
        "difficulty_options = ['Any'] + sorted(df['Difficulty'].unique())\n",
        "marks_options = ['Any'] + sorted(df['Marks'].unique())\n",
        "\n",
        "# Filter function\n",
        "def filter_questions(bloom, difficulty, marks):\n",
        "    filtered_df = df.copy()\n",
        "\n",
        "    if bloom != 'Any':\n",
        "        filtered_df = filtered_df[filtered_df[\"Bloom's Taxonomy Level\"] == bloom]\n",
        "    if difficulty != 'Any':\n",
        "        filtered_df = filtered_df[filtered_df['Difficulty'] == difficulty]\n",
        "    if marks != 'Any':\n",
        "        filtered_df = filtered_df[filtered_df['Marks'] == int(marks)]\n",
        "\n",
        "    if filtered_df.empty:\n",
        "        return \"❌ No questions match the selected filters.\"\n",
        "\n",
        "    # Build output string\n",
        "    output = \"\"\n",
        "    for idx, row in filtered_df.iterrows():\n",
        "        q_no = row['No']\n",
        "        question = row['Question']\n",
        "        bloom_level = row[\"Bloom's Taxonomy Level\"]\n",
        "        difficulty = row['Difficulty']\n",
        "        marks_val = row['Marks']\n",
        "        output += f\"🔸 **Q{q_no}** ({difficulty}, {marks_val} marks, {bloom_level}):\\n{question}\\n\\n\"\n",
        "\n",
        "    return output\n",
        "\n",
        "# Gradio UI\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"## 🔍 Filter and View Questions\")\n",
        "\n",
        "    with gr.Row():\n",
        "        bloom_dd = gr.Dropdown(choices=bloom_options, label=\"Bloom's Taxonomy Level\", value='Any')\n",
        "        diff_dd = gr.Dropdown(choices=difficulty_options, label=\"Difficulty\", value='Any')\n",
        "        marks_dd = gr.Dropdown(choices=marks_options, label=\"Marks\", value='Any')\n",
        "\n",
        "    output_box = gr.Markdown()\n",
        "    btn = gr.Button(\"🔎 Show Questions\")\n",
        "\n",
        "    btn.click(fn=filter_questions,\n",
        "              inputs=[bloom_dd, diff_dd, marks_dd],\n",
        "              outputs=output_box)\n",
        "\n",
        "# Launch the app\n",
        "demo.launch(share=True)\n"
      ],
      "metadata": {
        "id": "_aBS-hbO_aCP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''def generate_answer_key_with_mistral(questions_output, context):\n",
        "    prompt = f\"\"\"\n",
        "You are an AI assistant specialized in answering technical questions.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Questions:\n",
        "{questions_output}\n",
        "\n",
        "Generate answers for each questions listed above:\n",
        "\n",
        "### Answer Key:\n",
        "\"\"\"\n",
        "    inputs = mistral_tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=4096).to(\"cuda\")\n",
        "    output = mistral_model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=4096,\n",
        "        temperature=0.7,\n",
        "        top_p=0.9,\n",
        "        do_sample=True,\n",
        "        pad_token_id=mistral_tokenizer.eos_token_id\n",
        "    )\n",
        "    generated_text = mistral_tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    print(generated_text)\n",
        "    return generated_text.split(\"### Answer Key:\")[-1].strip()\n",
        "\n",
        "answers_raw = generate_answer_key_with_mistral(questions_raw, final_full_texts)\n",
        "\n",
        "print(\"Answers:\\n\", answers_raw)\n",
        "#query = input(\"\\nEnter your query: \")\n",
        "bulk_process_pages(final_full_texts, query, batch_size=5)\n",
        "'''"
      ],
      "metadata": {
        "id": "brjYQb4mjRa9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''import re\n",
        "import csv\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from google.colab import files\n",
        "\n",
        "# Bloom's Taxonomy Labels\n",
        "bloom_labels = {\n",
        "    0: \"BT1 (Remembering)\",\n",
        "    1: \"BT2 (Understanding)\",\n",
        "    2: \"BT3 (Applying)\",\n",
        "    3: \"BT4 (Analyzing)\",\n",
        "    4: \"BT5 (Evaluating)\",\n",
        "    5: \"BT6 (Creating)\"\n",
        "}\n",
        "\n",
        "difficulty_labels = {\n",
        "    0: \"Easy\",\n",
        "    1: \"Medium\",\n",
        "    2: \"Hard\"\n",
        "}\n",
        "\n",
        "mark_labels = {\n",
        "    0: \"2 Marks\",\n",
        "    1: \"4 Marks\",\n",
        "    2: \"8 Marks\"\n",
        "}\n",
        "\n",
        "# Utility functions\n",
        "def extract_numbered_list(text):\n",
        "    items = re.split(r'\\n(?=\\d+\\.\\s)', text.strip())\n",
        "    return [item.strip() for item in items if item.strip()]\n",
        "\n",
        "def split_number_and_text(item):\n",
        "    match = re.match(r'^(\\d+)\\.\\s+(.*)', item, re.DOTALL)\n",
        "    if match:\n",
        "        return int(match.group(1)), match.group(2).strip()\n",
        "    return None, item.strip()\n",
        "\n",
        "# Mistral Question Generation\n",
        "def generate_questions_with_mistral_bulk(pages_text, user_query):\n",
        "    prompt = f\"\"\"\n",
        "You are an AI assistant specialized in question generation.\n",
        "Your goal is to generate insightful questions based on the given context and user query.\n",
        "\n",
        "Context:\n",
        "{pages_text}\n",
        "\n",
        "User Query (Focus Topic): {user_query}\n",
        "\n",
        "### Reasoning:\n",
        "- Step 1: Identify key points and concepts from the context relevant to the query\n",
        "- Step 2: Consider what types of questions best explore the topic of interest\n",
        "- Step 3: Formulate meaningful and topic-specific questions\n",
        "\n",
        "### Questions:\n",
        "\"\"\"\n",
        "    inputs = mistral_tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=4096).to(\"cuda\")\n",
        "    output = mistral_model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=1024,\n",
        "        temperature=0.7,\n",
        "        top_p=0.9,\n",
        "        do_sample=True,\n",
        "        pad_token_id=mistral_tokenizer.eos_token_id\n",
        "    )\n",
        "    generated_text = mistral_tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    print(generated_text.split(\"### Questions:\")[-1].strip())\n",
        "    return generated_text.split(\"### Questions:\")[-1].strip()\n",
        "\n",
        "\n",
        "# Mistral Answer Generation\n",
        "def generate_answer_key_with_mistral(questions_output, context):\n",
        "    prompt = f\"\"\"\n",
        "You are an AI assistant specialized in answering technical questions.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Questions:\n",
        "{questions_output}\n",
        "\n",
        "### Answer Key:\n",
        "\"\"\"\n",
        "    inputs = mistral_tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=4096).to(\"cuda\")\n",
        "    output = mistral_model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=2048,\n",
        "        temperature=0.7,\n",
        "        top_p=0.9,\n",
        "        do_sample=True,\n",
        "        pad_token_id=mistral_tokenizer.eos_token_id\n",
        "    )\n",
        "    generated_text = mistral_tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    print(generated_text.split(\"### Answer Key:\")[-1].strip())\n",
        "    return generated_text.split(\"### Answer Key:\")[-1].strip()\n",
        "\n",
        "# Bloom’s Classifier\n",
        "def classify_blooms_taxonomy(question):\n",
        "    inputs = blooms_tokenizer(question, return_tensors=\"pt\", truncation=True, padding=True).to(\"cuda\")\n",
        "    with torch.no_grad():\n",
        "        outputs = blooms_model(**inputs)\n",
        "    probs = F.softmax(outputs.logits, dim=1).squeeze().tolist()\n",
        "    predicted_idx = torch.argmax(outputs.logits, dim=1).item()\n",
        "    predicted_label = bloom_labels[predicted_idx]\n",
        "    prob_dict = {bloom_labels[i]: round(probs[i], 4) for i in range(6)}\n",
        "    return predicted_label, prob_dict\n",
        "\n",
        "def classify_difficulty(question):\n",
        "    inputs = difficulty_tokenizer(question, return_tensors=\"pt\", truncation=True, padding=True).to(\"cuda\")\n",
        "    with torch.no_grad():\n",
        "        outputs = difficulty_model(**inputs)\n",
        "    probs = F.softmax(outputs.logits, dim=1).squeeze().tolist()\n",
        "    predicted_idx = torch.argmax(outputs.logits, dim=1).item()\n",
        "    predicted_label = difficulty_labels[predicted_idx]\n",
        "    prob_dict = {difficulty_labels[i]: round(probs[i], 4) for i in range(3)}\n",
        "    return predicted_label, prob_dict\n",
        "\n",
        "def classify_marks(question):\n",
        "    inputs = marks_tokenizer(question, return_tensors=\"pt\", truncation=True, padding=True).to(\"cuda\")\n",
        "    with torch.no_grad():\n",
        "        outputs = marks_model(**inputs)\n",
        "    probs = F.softmax(outputs.logits, dim=1).squeeze().tolist()\n",
        "    predicted_idx = torch.argmax(outputs.logits, dim=1).item()\n",
        "    predicted_label = mark_labels[predicted_idx]\n",
        "    prob_dict = {mark_labels[i]: round(probs[i], 4) for i in range(3)}\n",
        "    return predicted_label, prob_dict\n",
        "\n",
        "def bulk_process_pages(pages_array, query, batch_size, output_csv=\"final_questions_classified.csv\"):\n",
        "    all_qa_pairs = []\n",
        "\n",
        "    for i in range(0, len(pages_array), batch_size):\n",
        "        batch_pages = pages_array[i:i+batch_size]\n",
        "        combined_text = \"\\n\".join(page[\"text\"] for page in batch_pages)\n",
        "        print(f\"\\n📄 Processing Pages {i+1} to {min(i+batch_size, len(pages_array))}...\")\n",
        "\n",
        "        if len(mistral_tokenizer.tokenize(combined_text)) > 4096:\n",
        "            print(\"⚠️ Combined text exceeds token limit. Consider reducing batch size.\")\n",
        "            continue\n",
        "\n",
        "        questions = generate_questions_with_mistral_bulk(combined_text, query)\n",
        "        answers = generate_answer_key_with_mistral(questions_output=questions, context=combined_text)\n",
        "\n",
        "        question_items = extract_numbered_list(questions)\n",
        "        answer_items = extract_numbered_list(answers)\n",
        "\n",
        "        for q_item, a_item in zip(question_items, answer_items):\n",
        "            q_num, q_text = split_number_and_text(q_item)\n",
        "            a_num, a_text = split_number_and_text(a_item)\n",
        "\n",
        "            if q_num == a_num and q_text and a_text:\n",
        "                print(f\"\\nQ{q_num}: {q_text}\\nA{a_num}: {a_text}\")\n",
        "                all_qa_pairs.append((q_num, q_text, a_text))\n",
        "            else:\n",
        "                print(f\"⚠️ Mismatch: Question {q_num} doesn't match Answer {a_num}\")\n",
        "\n",
        "    with open(output_csv, mode='w', newline='', encoding='utf-8') as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow([\n",
        "            \"No\", \"Question\", \"Bloom's Taxonomy Level\", \"Difficulty Level\", \"Marks\",\n",
        "            \"Answer\",\n",
        "            \"BT1 (Remembering)\", \"BT2 (Understanding)\", \"BT3 (Applying)\",\n",
        "            \"BT4 (Analyzing)\", \"BT5 (Evaluating)\", \"BT6 (Creating)\",\n",
        "            \"Easy\", \"Medium\", \"Hard\",\n",
        "            \"2 Marks\", \"4 Marks\", \"8 Marks\"\n",
        "        ])\n",
        "\n",
        "        for idx, (num, question, answer) in enumerate(all_qa_pairs, 1):\n",
        "            bloom_level, bloom_probs = classify_blooms_taxonomy(question)\n",
        "            difficulty_level, diff_probs = classify_difficulty(question)\n",
        "            marks_level, marks_probs = classify_marks(question)\n",
        "\n",
        "            writer.writerow([\n",
        "                idx, question, bloom_level, difficulty_level, marks_level,\n",
        "                answer,\n",
        "                bloom_probs[\"BT1 (Remembering)\"], bloom_probs[\"BT2 (Understanding)\"],\n",
        "                bloom_probs[\"BT3 (Applying)\"], bloom_probs[\"BT4 (Analyzing)\"],\n",
        "                bloom_probs[\"BT5 (Evaluating)\"], bloom_probs[\"BT6 (Creating)\"],\n",
        "                diff_probs[\"Easy\"], diff_probs[\"Medium\"], diff_probs[\"Hard\"],\n",
        "                marks_probs[\"2 Marks\"], marks_probs[\"4 Marks\"], marks_probs[\"8 Marks\"]\n",
        "            ])\n",
        "\n",
        "    print(f\"\\n✅ Saved {len(all_qa_pairs)} classified Q&A pairs to '{output_csv}'\")\n",
        "    files.download(output_csv)\n",
        "'''"
      ],
      "metadata": {
        "id": "C5_hP25kw6LI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "def extract_numbered_list(text):\n",
        "    \"\"\"Extracts numbered items (e.g., 1. ... 2. ...) from a single string.\"\"\"\n",
        "    items = re.split(r'\\n(?=\\d+\\.\\s)', text.strip())\n",
        "    return [item.strip() for item in items if item.strip()]\n",
        "\n",
        "def split_number_and_text(item):\n",
        "    \"\"\"Splits '1. Some text here' into (1, 'Some text here')\"\"\"\n",
        "    match = re.match(r'^(\\d+)\\.\\s+(.*)', item, re.DOTALL)\n",
        "    if match:\n",
        "        return int(match.group(1)), match.group(2).strip()\n",
        "    return None, item.strip()\n",
        "\n",
        "def generate_questions_with_mistral_bulk(pages_text, user_query):\n",
        "    prompt = f\"\"\"\n",
        "You are an AI assistant specialized in question generation.\n",
        "Your goal is to generate insightful questions based on the given context and user query.\n",
        "\n",
        "Please follow these steps:\n",
        "1. Analyze the provided context to identify key points related to the user query.\n",
        "2. Focus on the topic specified by the user while framing the questions.\n",
        "3. Generate questions first.\n",
        "\n",
        "Context:\n",
        "{pages_text}\n",
        "\n",
        "User Query (Focus Topic): {user_query}\n",
        "\n",
        "### Reasoning:\n",
        "- Step 1: Identify key points and concepts from the context relevant to the query\n",
        "- Step 2: Consider what types of questions best explore the topic of interest\n",
        "- Step 3: Formulate meaningful and topic-specific questions\n",
        "\n",
        "### Questions:\n",
        "\"\"\"\n",
        "\n",
        "    inputs = mistral_tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=4096).to(\"cuda\")\n",
        "\n",
        "    output = mistral_model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=4096,\n",
        "        temperature=0.7,\n",
        "        top_p=0.9,\n",
        "        do_sample=True,\n",
        "        pad_token_id=mistral_tokenizer.eos_token_id\n",
        "    )\n",
        "    generated_text = mistral_tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    #print(generated_text)\n",
        "    return generated_text.split(\"### Questions:\")[-1].strip()\n",
        "\n",
        "\n",
        "\n",
        "def generate_answer_key_with_mistral(questions_output, context):\n",
        "    prompt = f\"\"\"\n",
        "You are an AI assistant specialized in answering technical questions.\n",
        "\n",
        "Please use the following context to generate a precise answer key for each question.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Questions:\n",
        "{questions_output}\n",
        "\n",
        "\n",
        "### Answer Key:\n",
        "\"\"\"\n",
        "\n",
        "    inputs = mistral_tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=4096).to(\"cuda\")\n",
        "\n",
        "    output = mistral_model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=4096,\n",
        "        temperature=0.7,\n",
        "        top_p=0.9,\n",
        "        do_sample=True,\n",
        "        pad_token_id=mistral_tokenizer.eos_token_id\n",
        "    )\n",
        "    generated_text = mistral_tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    #print(generated_text)\n",
        "    return generated_text.split(\"### Answer Key:\")[-1].strip()\n",
        "\n",
        "\n",
        "\n",
        "def generate_questions_and_answers(pages_text, user_query,csv_filename=\"generated_questions_answers.csv\"):\n",
        "    questions = generate_questions_with_mistral_bulk(pages_text, user_query)\n",
        "    answers = generate_answer_key_with_mistral(questions_output=questions, context=pages_text)\n",
        "    print(questions)\n",
        "    print(answers)\n",
        "    question_items = extract_numbered_list(questions)\n",
        "    answer_items = extract_numbered_list(answers)\n",
        "\n",
        "    # Step 3: Map question and answer numbers\n",
        "    qa_pairs = []\n",
        "    for q_item, a_item in zip(question_items, answer_items):\n",
        "        q_num, q_text = split_number_and_text(q_item)\n",
        "        a_num, a_text = split_number_and_text(a_item)\n",
        "\n",
        "        if q_num == a_num:\n",
        "            qa_pairs.append((q_num, q_text, a_text))\n",
        "        else:\n",
        "            print(f\"⚠️ Mismatch: Question {q_num} doesn't match Answer {a_num}\")\n",
        "\n",
        "    # Step 4: Write to CSV\n",
        "    with open(csv_filename, mode='w', newline='', encoding='utf-8') as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow(['No', 'Question', 'Answer'])\n",
        "\n",
        "        for num, question, answer in qa_pairs:\n",
        "            writer.writerow([num, question, answer])\n",
        "\n",
        "    print(f\"\\n✅ Saved {len(qa_pairs)} Q&A pairs to '{csv_filename}'\")\n",
        "\n",
        "    files.download(csv_filename)\n",
        "\n",
        "\n",
        "\n",
        "import torch.nn.functional as F\n",
        "bloom_labels = {\n",
        "    0: \"BT1 (Remembering)\",\n",
        "    1: \"BT2 (Understanding)\",\n",
        "    2: \"BT3 (Applying)\",\n",
        "    3: \"BT4 (Analyzing)\",\n",
        "    4: \"BT5 (Evaluating)\",\n",
        "    5: \"BT6 (Creating)\"\n",
        "}\n",
        "\n",
        "def classify_blooms_taxonomy(question):\n",
        "    inputs = blooms_tokenizer(question, return_tensors=\"pt\", truncation=True, padding=True).to(\"cuda\")\n",
        "    with torch.no_grad():\n",
        "        outputs = blooms_model(**inputs)\n",
        "    probs = F.softmax(outputs.logits, dim=1).squeeze().tolist()\n",
        "\n",
        "    predicted_idx = torch.argmax(outputs.logits, dim=1).item()\n",
        "    predicted_label = bloom_labels[predicted_idx]\n",
        "\n",
        "    prob_dict = {bloom_labels[i]: round(probs[i], 4) for i in range(6)}\n",
        "    return predicted_label, prob_dict\n",
        "\n",
        "def classify_questions_and_save(input_csv=\"generated_questions_answers.csv\", output_csv=\"final_questions_classified.csv\"):\n",
        "    with open(input_csv, mode='r', encoding='utf-8') as infile, open(output_csv, mode='w', newline='', encoding='utf-8') as outfile:\n",
        "        reader = csv.reader(infile)\n",
        "        writer = csv.writer(outfile)\n",
        "\n",
        "        # Write header\n",
        "        writer.writerow([\n",
        "            \"No\",\"Question\", \"Answer\", \"Bloom's Taxonomy Level\",\n",
        "            \"BT1 (Remembering)\", \"BT2 (Understanding)\", \"BT3 (Applying)\",\n",
        "            \"BT4 (Analyzing)\", \"BT5 (Evaluating)\", \"BT6 (Creating)\"\n",
        "        ])\n",
        "\n",
        "        next(reader)  # Skip header\n",
        "\n",
        "        for row in reader:\n",
        "            no,question,answer = row[0],row[1],row[2]\n",
        "            bloom_level, probs = classify_blooms_taxonomy(question)\n",
        "\n",
        "            writer.writerow([\n",
        "                no,question, answer, bloom_level,\n",
        "                probs[\"BT1 (Remembering)\"], probs[\"BT2 (Understanding)\"],\n",
        "                probs[\"BT3 (Applying)\"], probs[\"BT4 (Analyzing)\"],\n",
        "                probs[\"BT5 (Evaluating)\"], probs[\"BT6 (Creating)\"]\n",
        "            ])\n",
        "\n",
        "    from google.colab import files\n",
        "    files.download(output_csv)\n",
        "\n",
        "\n",
        "\n",
        "def bulk_process_pages(pages_array,query,batch_size):\n",
        "\n",
        "\n",
        "\n",
        "      for i in range(0, len(pages_array), batch_size):\n",
        "        batch_pages = pages_array[i:i+batch_size]  # Get a batch of pages\n",
        "        combined_text = \"\\n\".join(page[\"text\"] for page in batch_pages)\n",
        "        print(f\"📄 Processing Pages {i+1} to {min(i+batch_size, len(pages_array))}...\")\n",
        "\n",
        "        # Generate questions with CoT\n",
        "\n",
        "        generate_questions_and_answers(combined_text,query)\n",
        "        classify_questions_and_save()\n",
        "\n",
        "\n",
        "\n",
        "query = input(\"\\nEnter your query: \")\n",
        "bulk_process_pages(final_full_texts,query,batch_size=5)\n",
        "'''\n"
      ],
      "metadata": {
        "id": "qIUUrR8CkT1U"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}