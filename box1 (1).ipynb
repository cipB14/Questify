{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": true,
        "id": "TmRX-d1dUkYM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f3f49f7-2c89-42ba-aba5-8cd35d3cbedc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m20.0/20.0 MB\u001b[0m \u001b[31m37.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m62.3/62.3 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m322.1/322.1 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m11.2/11.2 MB\u001b[0m \u001b[31m58.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -qU pdf4llm pymupdf gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "2pAEbupIUT_U"
      },
      "outputs": [],
      "source": [
        "import pdf4llm\n",
        "import fitz  # PyMuPDF\n",
        "import gradio as gr\n",
        "from google.colab import files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "V-Wn1_gYvkGm",
        "outputId": "25b1f8e6-5e02-4df8-e3f4-b967b7bb251d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    To log in, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Enter your token (input will not be visible): \n",
            "Add token as git credential? (Y/n) Y\n",
            "Token is valid (permission: fineGrained).\n",
            "The token `cipb14` has been saved to /root/.cache/huggingface/stored_tokens\n",
            "\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n",
            "You might have to re-authenticate when pushing to the Hugging Face Hub.\n",
            "Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n",
            "\n",
            "git config --global credential.helper store\n",
            "\n",
            "Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n",
            "Token has not been saved to git credential helper.\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful.\n",
            "The current active token is: `cipb14`\n"
          ]
        }
      ],
      "source": [
        "!huggingface-cli login\n",
        "#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\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install -qU transformers accelerate bitsandbytes\n",
        "\n",
        "import torch\n",
        "from google.colab import files\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BertTokenizer, BertForSequenceClassification, BitsAndBytesConfig\n",
        "\n",
        "\n",
        "# 1ï¸âƒ£ Enable 4-bit Quantization\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,  # Use 4-bit precision\n",
        "    bnb_4bit_compute_dtype=\"float16\",  # Use float16 for faster computation\n",
        "    bnb_4bit_use_double_quant=True,  # Improves efficiency\n",
        "    bnb_4bit_quant_type=\"nf4\"  # NF4 quantization for better accuracy\n",
        ")\n",
        "\n",
        "# 2ï¸âƒ£ Load Tokenizer & Model with Quantization\n",
        "mistral_tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.3\")\n",
        "\n",
        "mistral_model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"mistralai/Mistral-7B-Instruct-v0.3\",\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"cuda\"  # Automatically assigns model to GPU\n",
        ")\n",
        "\n",
        "# ğŸ”¹ Load Bloomâ€™s Taxonomy BERT Model\n",
        "blooms_model_name = \"cip29/blooms_bert\"  # Update with your model name if different\n",
        "blooms_tokenizer = BertTokenizer.from_pretrained(blooms_model_name)\n",
        "blooms_model = BertForSequenceClassification.from_pretrained(blooms_model_name, num_labels=6).to(\"cuda\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "id": "b027a2FIl6Dt",
        "outputId": "8d2be625-f444-45e1-e2be-600d87285544"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "module functions cannot set METH_CLASS or METH_STATIC",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-438144875801>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoModelForCausalLM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBertForSequenceClassification\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBitsAndBytesConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# Check the dependencies satisfy the minimal versions required.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdependency_versions_check\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m from .utils import (\n\u001b[1;32m     28\u001b[0m     \u001b[0mOptionalDependencyNotAvailable\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/dependency_versions_check.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdependency_versions_table\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdeps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrequire_version\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequire_version_core\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__version__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbackbone_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBackboneConfigMixin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBackboneMixin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mchat_template_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDocstringParsingException\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTypeHintParsingException\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_json_schema\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mconstants\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mIMAGENET_DEFAULT_MEAN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIMAGENET_DEFAULT_STD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIMAGENET_STANDARD_MEAN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIMAGENET_STANDARD_STD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m from .doc import (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/chat_template_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_torch_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    365\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mUSE_GLOBAL_DEPS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m         \u001b[0m_load_global_deps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 367\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa: F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: module functions cannot set METH_CLASS or METH_STATIC"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "EJ9pIGmMZ296"
      },
      "outputs": [],
      "source": [
        "import pdf4llm\n",
        "from google.colab import files\n",
        "\n",
        "# 1ï¸âƒ£ Upload PDF\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Get the uploaded filename\n",
        "pdf_filename = list(uploaded.keys())[0]\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 2ï¸âƒ£ Ask user for page range or specific pages\n",
        "page_input = input(\"\\nEnter page numbers or range (e.g., 1-5 or 2,4,6): \")\n",
        "\n",
        "# 3ï¸âƒ£ Convert input into a list of page indices (zero-based)\n",
        "selected_pages = []\n",
        "for part in page_input.split(\",\"):\n",
        "    part = part.strip()\n",
        "    if \"-\" in part:  # Handle range (e.g., \"2-5\")\n",
        "        start, end = map(int, part.split(\"-\"))\n",
        "        selected_pages.extend(range(start - 1, end))  # Convert to zero-based index\n",
        "    else:  # Handle single page (e.g., \"3\")\n",
        "        selected_pages.append(int(part) - 1)  # Convert to zero-based index\n",
        "\n",
        "# 4ï¸âƒ£ Extract only the specified pages\n",
        "selected_page_data = pdf4llm.to_markdown(pdf_filename, page_chunks=True, pages=selected_pages)\n",
        "\n",
        "# 5ï¸âƒ£ Store page-wise text in an array\n",
        "pages_array = [page[\"text\"] for page in selected_page_data]  # Ensure correct key\n",
        "\n",
        "# 6ï¸âƒ£ Display Extracted Text (First 500 chars of each selected page)\n",
        "print(\"\\nğŸ”¹ Extracted Pages:\\n\")\n",
        "for i, page_text in enumerate(pages_array):\n",
        "    print(f\"\\nğŸ“„ Page {selected_pages[i] + 1} \\n\")\n",
        "    print(page_text[:500])  # Show first 500 characters"
      ],
      "metadata": {
        "id": "JW5ejoZGhisn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2tTpF3vVcZ1C"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ğŸ”¹ Function 1: Generate Questions with Mistral-7B\n",
        "def generate_questions_with_mistral(page_text):\n",
        "    prompt = f'''You are an AI assistant specialized in question generation.\n",
        "Before generating the questions, let's reason through the context to create thoughtful questions.\n",
        "Please follow these steps:\n",
        "\n",
        "1. Analyze the provided context and identify the main topics or key points.\n",
        "2. Think about the types of questions that are typically asked about this subject.\n",
        "3. Consider what the reader should understand about this context in order to generate insightful questions.\n",
        "\n",
        "Context:\n",
        "{page_text}\n",
        "\n",
        "### Reasoning:\n",
        "- Step 1: Identifying key points and main topics\n",
        "- Step 2: Considering what kinds of questions would be useful\n",
        "- Step 3: Formulating questions based on the key points and topics\n",
        "\n",
        "### Questions:\n",
        "'''\n",
        "\n",
        "    # Tokenize input\n",
        "    inputs = mistral_tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "    # Generate output\n",
        "    output = mistral_model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=500,\n",
        "        temperature=0.7,\n",
        "        top_p=0.9,\n",
        "        do_sample=True,\n",
        "        pad_token_id=mistral_tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "    generated_text = mistral_tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "    # Extract the questions after the reasoning\n",
        "    if \"### Questions:\" in generated_text:\n",
        "        generated_questions = generated_text.split(\"### Questions:\")[1].strip()\n",
        "    else:\n",
        "        generated_questions = generated_text  # Fallback in case the marker is missing\n",
        "\n",
        "    return generated_questions.split(\"\\n\")  # Split questions into a list\n",
        "\n",
        "\n",
        "# ğŸ”¹ Function 2: Classify Questions using Bloomâ€™s Taxonomy BERT\n",
        "def classify_blooms_taxonomy(question):\n",
        "    inputs = blooms_tokenizer(question, return_tensors=\"pt\", padding=True, truncation=True).to(\"cuda\")\n",
        "    with torch.no_grad():\n",
        "        outputs = blooms_model(**inputs)\n",
        "\n",
        "    probs = torch.nn.functional.softmax(outputs.logits, dim=1).cpu().numpy().flatten()\n",
        "    bloom_levels = [\"BT1 (Remembering)\", \"BT2 (Understanding)\", \"BT3 (Applying)\",\n",
        "                    \"BT4 (Analyzing)\", \"BT5 (Evaluating)\", \"BT6 (Creating)\"]\n",
        "\n",
        "    # Get highest probability Bloom level\n",
        "    max_index = probs.argmax()\n",
        "    predicted_level = bloom_levels[max_index]\n",
        "\n",
        "    # Store probabilities\n",
        "    probabilities_dict = {bloom_levels[i]: probs[i] for i in range(len(bloom_levels))}\n",
        "\n",
        "    return predicted_level, probabilities_dict\n",
        "\n",
        "\n",
        "# ğŸ”¹ Function 3: Generate and Save Questions to CSV\n",
        "def generate_and_save_questions(pages_array, filename=\"generated_questions.csv\"):\n",
        "    with open(filename, mode='w', newline='', encoding='utf-8') as file:\n",
        "        writer = csv.writer(file)\n",
        "\n",
        "        # Write header row\n",
        "        writer.writerow([\"Page Number\", \"Question\"])\n",
        "\n",
        "        # Process each page\n",
        "        for i, page_text in enumerate(pages_array):\n",
        "            print(f\"ğŸ“„ Generating Questions for Page {i+1}...\")\n",
        "\n",
        "            # Generate questions\n",
        "            questions = generate_questions_with_mistral(page_text)\n",
        "\n",
        "            # Write questions to CSV\n",
        "            for question in questions:\n",
        "                writer.writerow([f\"Page {i+1}\", question])\n",
        "\n",
        "    # ğŸ“¥ Download CSV file\n",
        "    files.download(filename)\n",
        "\n",
        "\n",
        "# ğŸ”¹ Function 4: Classify Questions & Save with Bloomâ€™s Taxonomy\n",
        "def classify_questions_and_save(input_csv=\"generated_questions.csv\", output_csv=\"classified_questions.csv\"):\n",
        "    with open(input_csv, mode='r', encoding='utf-8') as infile, open(output_csv, mode='w', newline='', encoding='utf-8') as outfile:\n",
        "        reader = csv.reader(infile)\n",
        "        writer = csv.writer(outfile)\n",
        "\n",
        "        # Write header row\n",
        "        writer.writerow([\"Page Number\", \"Question\", \"Bloom's Taxonomy Level\",\n",
        "                         \"BT1 (Remembering)\", \"BT2 (Understanding)\", \"BT3 (Applying)\",\n",
        "                         \"BT4 (Analyzing)\", \"BT5 (Evaluating)\", \"BT6 (Creating)\"])\n",
        "\n",
        "        next(reader)  # Skip header\n",
        "\n",
        "        for row in reader:\n",
        "            page_num, question = row\n",
        "            bloom_level, probs = classify_blooms_taxonomy(question)\n",
        "\n",
        "            # Write to output CSV\n",
        "            writer.writerow([page_num, question, bloom_level,\n",
        "                             probs[\"BT1 (Remembering)\"], probs[\"BT2 (Understanding)\"],\n",
        "                             probs[\"BT3 (Applying)\"], probs[\"BT4 (Analyzing)\"],\n",
        "                             probs[\"BT5 (Evaluating)\"], probs[\"BT6 (Creating)\"]])\n",
        "\n",
        "    # ğŸ“¥ Download CSV file\n",
        "    files.download(output_csv)\n",
        "\n",
        "\n",
        "# ğŸ”¹ Example Usage:\n",
        "# Step 1: Generate questions and save them\n",
        "generate_and_save_questions(pages_array)\n",
        "\n",
        "# Step 2: Classify questions and save them with Bloomâ€™s levels\n",
        "classify_questions_and_save()"
      ],
      "metadata": {
        "id": "1D0j3RAwrQTz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU transformers accelerate bitsandbytes\n",
        "import csv\n",
        "from google.colab import files\n",
        "\n",
        "def generate_questions_with_cot(page_text):\n",
        "    prompt = f'''You are an AI assistant specialized in question generation.\n",
        "Before generating the questions, let's reason through the context to create thoughtful questions.\n",
        "Please follow these steps:\n",
        "\n",
        "1. Analyze the provided context and identify the main topics or key points.\n",
        "2. Think about the types of questions that are typically asked about this subject.\n",
        "3. Consider what the reader should understand about this context in order to generate insightful questions.\n",
        "\n",
        "Context:\n",
        "{page_text}\n",
        "\n",
        "### Reasoning:\n",
        "- Step 1: Identifying key points and main topics\n",
        "- Step 2: Considering what kinds of questions would be useful\n",
        "- Step 3: Formulating questions based on the key points and topics\n",
        "\n",
        "### Questions:\n",
        "'''\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "    output = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=500,\n",
        "        temperature=0.7,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "        do_sample=True\n",
        "    )\n",
        "\n",
        "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "    # Extract the questions after the reasoning\n",
        "    if \"### Questions:\" in generated_text:\n",
        "        generated_questions = generated_text.split(\"### Questions:\")[1].strip()\n",
        "    else:\n",
        "        generated_questions = generated_text  # Fallback in case the marker is missing\n",
        "\n",
        "    return generated_questions\n",
        "\n",
        "\n",
        "def generate_response(prompt):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")  # Send input to GPU\n",
        "    input_tokens = inputs[\"input_ids\"].shape[1]  # Get input token count\n",
        "    print(f\"Input token count: {input_tokens}\")\n",
        "\n",
        "    output = model.generate(**inputs, max_new_tokens=10000, temperature=1.0, pad_token_id=tokenizer.eos_token_id, do_sample=True)\n",
        "\n",
        "    output_tokens = output.shape[1]  # Get output token count\n",
        "    print(f\"Output token count: {output_tokens}\")\n",
        "\n",
        "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "\n",
        "def save_questions_to_csv(pages_array, filename=\"generated_questions.csv\"):\n",
        "    # Open a CSV file to write the results\n",
        "    with open(filename, mode='w', newline='', encoding='utf-8') as file:\n",
        "        writer = csv.writer(file)\n",
        "\n",
        "        # Write header row\n",
        "        writer.writerow([\"Page Number\", \"Generated Questions\"])\n",
        "\n",
        "        # Generate questions for each page and write to CSV\n",
        "        for i, page_text in enumerate(pages_array[:2]):  # You can adjust the number of pages to process\n",
        "            print(f\"Generating questions for Page {i+1}...\")\n",
        "            questions = generate_questions_with_cot(page_text)\n",
        "\n",
        "            # Write the page number and generated questions to the CSV file\n",
        "            writer.writerow([f\"Page {i+1}\", questions])\n",
        "\n",
        "    # Download the CSV file to local machine (Colab)\n",
        "    files.download(filename)\n",
        "\n",
        "\n",
        "def generate_responses_from_csv(input_filename=\"generated_questions.csv\", output_filename=\"generated_questions_and_responses.csv\"):\n",
        "    # Read the CSV file containing generated questions\n",
        "    with open(input_filename, mode='r', newline='', encoding='utf-8') as file:\n",
        "        reader = csv.reader(file)\n",
        "        rows = list(reader)\n",
        "\n",
        "    # Create or overwrite the output CSV file to write responses\n",
        "    with open(output_filename, mode='w', newline='', encoding='utf-8') as file:\n",
        "        writer = csv.writer(file)\n",
        "\n",
        "        # Write header row\n",
        "        writer.writerow([\"Page Number\", \"Generated Questions\", \"Generated Responses\"])\n",
        "\n",
        "        # Iterate through each row (page number and questions)\n",
        "        for row in rows[1:]:  # Skip the header row\n",
        "            page_number, questions = row[0], row[1]\n",
        "\n",
        "            # Generate responses for each question in the \"Generated Questions\"\n",
        "            responses = []\n",
        "            for question in questions.split(\"\\n\"):\n",
        "                response = generate_response(question)\n",
        "                responses.append(response)\n",
        "\n",
        "            # Combine the responses into a single string\n",
        "            responses_str = \"\\n\".join(responses)\n",
        "\n",
        "            # Write the page number, generated questions, and responses to the new CSV file\n",
        "            writer.writerow([page_number, questions, responses_str])\n",
        "\n",
        "    # Download the CSV file with responses\n",
        "    files.download(output_filename)\n",
        "\n",
        "\n",
        "# Example usage:\n",
        "# Assuming you have already processed the PDF and have pages_array populated\n",
        "# First, generate the questions and save them to a CSV\n",
        "save_questions_to_csv(pages_array)\n",
        "\n",
        "# Then, after the CSV is generated, run the second function to generate responses\n",
        "generate_responses_from_csv(input_filename=\"generated_questions.csv\", output_filename=\"generated_questions_and_responses.csv\")\n"
      ],
      "metadata": {
        "id": "70clhgeg2Pbg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Write a full story of Adam and eve to maximum length\"\n",
        "print(generate_response(prompt))\n"
      ],
      "metadata": {
        "id": "nNacPmoRxjiS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g1onhBxXcE47"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wKgV_EddwmR3",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "\n",
        "# Install required libraries\n",
        "!pip install -qU transformers accelerate bitsandbytes\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "# 1ï¸âƒ£ Enable 4-bit Quantization\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,  # Use 4-bit precision\n",
        "    bnb_4bit_compute_dtype=\"float16\",  # Use float16 for faster computation\n",
        "    bnb_4bit_use_double_quant=True,  # Improves efficiency\n",
        "    bnb_4bit_quant_type=\"nf4\"  # NF4 quantization for better accuracy\n",
        ")\n",
        "\n",
        "# 2ï¸âƒ£ Load Tokenizer & Model with Quantization\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.3\")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"mistralai/Mistral-7B-Instruct-v0.3\",\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"cuda\"  # Automatically assigns model to GPU\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OjKgIDO_0DlL"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "from google.colab import files\n",
        "\n",
        "def generate_questions_with_cot(page_text):\n",
        "    prompt = f'''You are an AI assistant specialized in question generation.\n",
        "Before generating the questions, let's reason through the context to create thoughtful questions.\n",
        "Please follow these steps:\n",
        "\n",
        "1. Analyze the provided context and identify the main topics or key points.\n",
        "2. Think about the types of questions that are typically asked about this subject.\n",
        "3. Consider what the reader should understand about this context in order to generate insightful questions.\n",
        "\n",
        "Context:\n",
        "{page_text}\n",
        "\n",
        "### Reasoning:\n",
        "- Step 1: Identifying key points and main topics\n",
        "- Step 2: Considering what kinds of questions would be useful\n",
        "- Step 3: Formulating questions based on the key points and topics\n",
        "\n",
        "### Questions:\n",
        "'''\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "    output = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=500,\n",
        "        temperature=0.7,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "        do_sample=True\n",
        "    )\n",
        "\n",
        "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "    # Extract the questions after the reasoning\n",
        "    if \"### Questions:\" in generated_text:\n",
        "        generated_questions = generated_text.split(\"### Questions:\")[1].strip()\n",
        "    else:\n",
        "        generated_questions = generated_text  # Fallback in case the marker is missing\n",
        "\n",
        "    return generated_questions\n",
        "\n",
        "def save_questions_to_csv(pages_array, filename=\"generated_questions.csv\"):\n",
        "    # Open a CSV file to write the results\n",
        "    with open(filename, mode='w', newline='', encoding='utf-8') as file:\n",
        "        writer = csv.writer(file)\n",
        "\n",
        "        # Write header row\n",
        "        writer.writerow([\"Page Number\", \"Generated Questions\"])\n",
        "\n",
        "        # Generate questions for each page and write to CSV\n",
        "        for i, page_text in enumerate(pages_array[:]):  # You can adjust the number of pages to process\n",
        "            print(f\"Generating questions for Page {i+1}...\")\n",
        "            questions = generate_questions_with_cot(page_text)\n",
        "\n",
        "            # Write the page number and generated questions to the CSV file\n",
        "            writer.writerow([f\"Page {i+1}\", questions])\n",
        "\n",
        "    # Download the CSV file to local machine (Colab)\n",
        "    files.download(filename)\n",
        "\n",
        "# Example usage:\n",
        "# Assuming you have already processed the PDF and have pages_array populated\n",
        "save_questions_to_csv(pages_array)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}